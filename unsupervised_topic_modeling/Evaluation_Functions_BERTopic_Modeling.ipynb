{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banned-books/project_banned_books/blob/main/unsupervised_topic_modeling/Evaluation_Functions_BERTopic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount GDrive"
      ],
      "metadata": {
        "id": "-nnX6iIrmkDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt7sBEtemggF",
        "outputId": "601fec5b-8ee1-4f7a-f793-496b12727366"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/MaartenGr/BERTopic.git"
      ],
      "metadata": {
        "id": "fYGpaqhkM-Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "P2-3i2ohu24R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWu1otxGqZYH",
        "outputId": "d9a0d3d5-f6bd-41e3-9fcf-91aa96e42992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import data cleaning & manipulation libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from itertools import combinations\n",
        "\n",
        "# Import libraries for NLP work\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import MaximalMarginalRelevance, PartOfSpeech\n",
        "from scipy import linalg\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%matplotlib inline\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTopic Modeling | Custom Evaluation Functions\n"
      ],
      "metadata": {
        "id": "nfHay0ts6o1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the coherence score evaluation method\n",
        "\n",
        "We delved into the BERTopic docs and issues channel to find this Gensim CoherenceModel solution proposed by the author of BERTopic: https://github.com/MaartenGr/BERTopic/issues/90\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5VSCRfRn6oG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_coherence(docs, topics):\n",
        "  \"\"\"\n",
        "  Calculate the coherence of a BERTopic model.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  docs: per BERTopic docs\n",
        "  topics: the BERTopic topics\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  # Preprocess Documents\n",
        "  documents = pd.DataFrame({\"Document\": docs,\n",
        "                            \"ID\": range(len(docs)),\n",
        "                            \"Topic\": topics})\n",
        "\n",
        "  documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "  cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
        "\n",
        "  # Extract vectorizer and analyzer from BERTopic\n",
        "  vectorizer = topic_model.vectorizer_model\n",
        "  analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "  # Use .get_feature_names_out() if you get an error with .get_feature_names()\n",
        "  words = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # Extract features for Topic Coherence evaluation\n",
        "  tokens = [analyzer(doc) for doc in cleaned_docs]\n",
        "  dictionary = corpora.Dictionary(tokens)\n",
        "  corpus = [dictionary.doc2bow(token) for token in tokens]\n",
        "\n",
        "  # Extract words in each topic if they are non-empty and exist in the dictionary\n",
        "  topic_words = []\n",
        "\n",
        "  for topic in range(len(set(topics))-topic_model._outliers):\n",
        "      words = list(zip(*topic_model.get_topic(topic)))[0]\n",
        "      words = [word for word in words if word in dictionary.token2id]\n",
        "      topic_words.append(words)\n",
        "\n",
        "  topic_words = [words for words in topic_words if len(words) > 0]\n",
        "\n",
        "  # Evaluate Coherence\n",
        "  coherence_model = CoherenceModel(topics=topic_words, \n",
        "                                  texts=tokens, \n",
        "                                  corpus=corpus,\n",
        "                                  dictionary=dictionary, \n",
        "                                  coherence='c_v') # You may also use 'c_uci', 'c_npmi', or 'u_mass'. \n",
        "\n",
        "  coherence = coherence_model.get_coherence()\n",
        "\n",
        "  return coherence"
      ],
      "metadata": {
        "id": "TWPB6kCpiGCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the perplexity score evaluation method\n",
        "\n",
        "This took some time to figure out. We calculated the `log_perplexity` from the `probs` variable and then converted it back to a perplexity score."
      ],
      "metadata": {
        "id": "4b3YGLDQ61N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity():\n",
        "  \"\"\" Calculate the perplexity of a BERTopic model.\"\"\"\n",
        "  \n",
        "  log_perplexity = -1 * np.mean(np.log(np.sum(probs)))\n",
        "  perplexity = np.exp(log_perplexity)\n",
        "\n",
        "  return perplexity, log_perplexity"
      ],
      "metadata": {
        "id": "5-KS9BoKlIwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the proportion of unique words evaluation method\n",
        "\n",
        "We calculate the proportion of unique words in a series of topics (to aid with understanding topic diversity).\n",
        "\n",
        "This code was pieced together through many Stack Overflow posts around building topic model evaluations from scratch."
      ],
      "metadata": {
        "id": "fZcCYIdGLc20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def proportion_unique_words(topic_model, topk):\n",
        "    \"\"\"\n",
        "    Calculate the proportion of unique words.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    topic_model: fitted BERTopic model\n",
        "    topk: top k words on which the topic diversity will be computed\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    topics_list = topic_model.get_topics()\n",
        "\n",
        "    topics = [[words for words, _ in topic_model.get_topic(topic)] \n",
        "                  for topic in range(len(set(topics_list))-1)]\n",
        "\n",
        "    if topk > len(topics[0]):\n",
        "        raise Exception('Words in these topics are less than '+ str(topk))\n",
        "\n",
        "    else:\n",
        "        unique_words = set()\n",
        "\n",
        "        for topic in topics:\n",
        "            unique_words = unique_words.union(set(topic[:topk]))\n",
        "            \n",
        "        puw = len(unique_words) / (topk * len(topics))\n",
        "        \n",
        "        return puw\n"
      ],
      "metadata": {
        "id": "lhPTZ0iAG6aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the average pairwise Jaccard distance between topics (to aid with understanding topic diversity)."
      ],
      "metadata": {
        "id": "HS2lhdMjMPU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pairwise_jaccard_diversity(topic_model, topk):\n",
        "    '''\n",
        "    Calculate the average pairwise Jaccard distance between the topics \n",
        "  \n",
        "    Parameters\n",
        "    ----------\n",
        "    topic_model: fitted BERTopic model\n",
        "    topk: top k words on which the topic diversity will be computed\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pjd: average pairwise jaccard distance\n",
        "    '''\n",
        "\n",
        "    topics_list = topic_model.get_topics()\n",
        "    \n",
        "    topics = [[words for words, _ in topic_model.get_topic(topic)] \n",
        "                  for topic in range(len(set(topics_list))-1)]\n",
        "\n",
        "    dist = 0\n",
        "    count = 0\n",
        "\n",
        "    for list1, list2 in combinations(topics, 2):\n",
        "        js = 1 - len(set(list1).intersection(set(list2)))/len(set(list1).union(set(list2)))\n",
        "        dist = dist + js\n",
        "        count = count + 1\n",
        "\n",
        "    return dist/count"
      ],
      "metadata": {
        "id": "wYXbUdtCHFU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}