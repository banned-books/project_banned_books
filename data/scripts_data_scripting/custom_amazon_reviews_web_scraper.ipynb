{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLybRDDjMYqhus1Vjah1q2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banned-books/project_banned_books/blob/main/data/scripts_data_scripting/custom_amazon_reviews_web_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount GDrive"
      ],
      "metadata": {
        "id": "LCViLaRwRbBP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs3DwLV7RVdf",
        "outputId": "c23788de-13d2-4793-f368-2db0bf5c38ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "6TmoKIAXR8-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from requests_html import HTMLSession\n",
        "import time\n",
        "from bs4 import BeautifulSoup \n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "El-q8A3AR_SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Banned Books List with Amazon Urls"
      ],
      "metadata": {
        "id": "lSIpzfvESChm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Banned Book List with Amazon Links\n",
        "ala_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/original_data/pen_dataset.csv')\n",
        "\n",
        "# Keep certain columns\n",
        "ala_df = ala_df[['Title', 'Author', 'Type of Ban', 'State', 'District','Date of Challenge/Removal', 'Origin of Challenge', 'amazon_url']]\n",
        "\n",
        "# Remove duplicate books for web scraper\n",
        "ala_df = ala_df.drop_duplicates(subset=['Title'])\n",
        "\n",
        "# Remove books without Amazon reviews\n",
        "clean_df = ala_df.dropna(subset=['amazon_url'])\n",
        "\n",
        "# Add the unique Amazon book review urls to a list to feed into the web scraper\n",
        "urls = list(clean_df['amazon_url'])"
      ],
      "metadata": {
        "id": "_6Neh7KXSDIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The number of unique books with Amazon.com reviews is\", len(urls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SFUr6v-SEfi",
        "outputId": "1644d813-ed3e-42f9-c38e-f758e2e98d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unique books with Amazon.com reviews is 1547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape Amazon Reviews"
      ],
      "metadata": {
        "id": "WsNU_RLjSFoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Reviews:\n",
        "    \n",
        "    review_date_pattern = re.compile('(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?) \\d+, \\d{4}')\n",
        "    product_name_pattern = re.compile('^https:\\/{2}www.amazon.com\\/(.+)\\/product-reviews')\n",
        "    \n",
        "    def __init__(self, url) -> None:\n",
        "        \"\"\"Initialize a session.\"\"\"\n",
        "        \n",
        "        self.session = HTMLSession()\n",
        "        self.headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36'}\n",
        "        self.session.headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
        "        self.session.headers['Accept-Language'] = 'en-US,en;q=0.5'\n",
        "        self.session.headers['Connection'] = 'keep-alive'\n",
        "        self.session.headers['Upgrade-Insecure-Requests'] = '1'        \n",
        "        self.url = url\n",
        "        \n",
        "    def pagination(self, page):\n",
        "        \"\"\"Work through pagination.\"\"\"\n",
        "        \n",
        "        r = self.session.get(self.url + str(page))\n",
        "        print(self.url + str(page))\n",
        "\n",
        "        if not r.html.find('div[data-hook=review]'):\n",
        "            return False\n",
        "\n",
        "        else:\n",
        "            return r.html.find('div[data-hook=review]')\n",
        "\n",
        "    def parse(self, reviews, page):\n",
        "        \"\"\"Parse the html.\"\"\"\n",
        "        \n",
        "        total = []\n",
        "        \n",
        "        response = self.session.get(self.url + str(page))\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        review_list = soup.find('div', {'id': 'cm_cr-review_list' })  \n",
        "        product_reviews = review_list.find_all('div', {'data-hook': 'review'}) \n",
        "        product_name = self.product_name_pattern.search(self.url).group(1) if self.product_name_pattern.search(self.url) else ''\n",
        "\n",
        "        if not product_name:\n",
        "            print('url is invalid. Please check the url.')\n",
        "            product_name = self.url\n",
        "            return\n",
        "        else:\n",
        "            product_name = product_name.replace('-', ' ')\n",
        "        \n",
        "        for review in product_reviews:\n",
        "            \n",
        "            try:\n",
        "                title = review.find('a', {'data-hook': 'review-title'}).text.strip()\n",
        "            except:\n",
        "                print('No title')\n",
        "                break\n",
        "             \n",
        "            try:\n",
        "                body = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "            except: \n",
        "                print('No body')\n",
        "                break\n",
        "                \n",
        "            try:\n",
        "                rating = review.find('i', {'data-hook': 'review-star-rating'}).text\n",
        "            except:\n",
        "                print('No rating')\n",
        "                break\n",
        "                \n",
        "            try:\n",
        "                verified_purchase = True if review.find('span', {'data-hook': 'avp-badge'}) else False\n",
        "            except: \n",
        "                print('No verified purchase')\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                review_date = self.review_date_pattern.search(review.find('span', {'data-hook': 'review-date'}).text).group(0)\n",
        "            except:\n",
        "                print('No review date')\n",
        "                break\n",
        "                \n",
        "            data = {\n",
        "                'product_name': product_name,\n",
        "                'title': title,\n",
        "                'body': body,\n",
        "                'rating': rating,\n",
        "                'verified_purchase': verified_purchase,\n",
        "                'review_date': review_date\n",
        "            }\n",
        "\n",
        "            total.append(data)\n",
        "\n",
        "        \n",
        "        return total"
      ],
      "metadata": {
        "id": "NKPMlpjSSHH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    \n",
        "    dfs = [] \n",
        "    \n",
        "    for url in tqdm(urls):\n",
        "        \n",
        "        full_url = url + 'sortBy=recent&pageNumber='\n",
        "    \n",
        "        amz=Reviews(full_url)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for x in range(1,1000):\n",
        "\n",
        "            print('getting page ', x)\n",
        "            time.sleep(0.6)\n",
        "\n",
        "            reviews = amz.pagination(x)\n",
        "\n",
        "            if reviews is not False:\n",
        "                results.append(amz.parse(reviews, x))\n",
        "\n",
        "            else:\n",
        "                print('No more review pages.')\n",
        "                break\n",
        "\n",
        "        flat_list = [item for sublist in results for item in sublist]\n",
        "\n",
        "        df = pd.DataFrame(flat_list)\n",
        "        dfs.append(df)\n",
        "        \n",
        "    pd.concat(dfs).to_csv(r'out_amz_full.csv', index=False)"
      ],
      "metadata": {
        "id": "DbYh9O6hSLy2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}